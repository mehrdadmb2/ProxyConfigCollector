import requests
import os
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm

# Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ ÙˆØ±ÙˆØ¯ÛŒ Ø´Ø§Ù…Ù„ URLÙ‡Ø§
input_file = "DATA(Urls)\\Urls.txt"

# Ù…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
output_folder = "configs\\"  # Ù…Ø³ÛŒØ± Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ ØªØºÛŒÛŒØ± Ø¯Ù‡ÛŒØ¯

# Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ Ø®Ø±ÙˆØ¬ÛŒ Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯
os.makedirs(output_folder, exist_ok=True)

# Ø®ÙˆØ§Ù†Ø¯Ù† Ù„ÛŒØ³Øª URLÙ‡Ø§ Ø§Ø² ÙØ§ÛŒÙ„
try:
    with open(input_file, "r", encoding="utf-8") as file:
        urls = [line.strip() for line in file if line.strip()]
except FileNotFoundError:
    print(f"âŒ Error: Input file '{input_file}' not found.")
    exit(1)

# Ø´Ù…Ø§Ø±Ù†Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù†Ø§Ù…â€ŒÚ¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
success_count = 0
failed_count = 0
failed_urls = []

# ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ© URL
def process_url(url):
    global success_count, failed_count
    try:
        response = requests.get(url, timeout=10)
        if response.status_code == 200 and response.text.strip():
            file_path = f"{output_folder}{success_count}.txt"
            with open(file_path, "w", encoding="utf-8") as file:
                file.write(f"URL: {url}\n\n{response.text}")
            success_count += 1
        else:
            failed_count += 1
            failed_urls.append(url)
    except requests.RequestException:
        failed_count += 1
        failed_urls.append(url)

# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ThreadPoolExecutor Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ù…Ø²Ù…Ø§Ù†
with ThreadPoolExecutor(max_workers=10) as executor:
    # Ù¾Ø±Ø¯Ø§Ø²Ø´ URLÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù…ÙˆØ§Ø²ÛŒ
    list(tqdm(executor.map(process_url, urls), total=len(urls), desc="Processing URLs"))

# Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ
print(f"âœ… Successfully downloaded {success_count} configurations.")
print(f"âŒ Failed to download {failed_count} configurations.")
if failed_urls:
    print("ğŸš« Failed URLs:")
    for failed_url in failed_urls:
        print(f"   - {failed_url}")
